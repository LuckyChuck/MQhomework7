# Thinking1	参数共享指的是什么？ #
答：参数共享指的是共享权重，卷积核的权重对于图像不同位置来说其实是一样的，所以可以进行复用，好处是参数的数量大大减少，减轻运算负担。


# Thinking2	为什么会用到batch normalization ? #

答：是一种数据预处理的方式，因为对于数据来说，量纲是不一样的，这样的话，运算起来效果不是很好，尤其是对于神经网络学习来说，因为层级的原因，每一层的学习结果误差会影响下层的结果，所以错误相应的也会被放大。所以需要做bn，将输入值强行拉回到0-1的正态分布中来。


# Thinking3	使用dropout可以解决什么问题？ #

答：可以减少过拟合。
课上没有过多的提及dropout，自己从网上找的原理，做个笔记
（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变
（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

（3）然后继续重复这一过程：

. 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
. 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。
. 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。
不断重复这一过程。